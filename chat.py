import requests
import subprocess
import time

from config import OLLAMA_URL, OLLAMA_MODEL, LLM_PROMPT

# ---------------------------
# Ollama è¿›ç¨‹ç®¡ç†
# ---------------------------
_ollama_process = None
_ollama_started_by_app = False


def _start_ollama_if_needed():
    """
    å¦‚æœç”¨æˆ·æœ¬åœ°å·²ç»åœ¨è¿è¡Œ ollama serveï¼Œåˆ™ä¸ä¼šé‡å¤å¯åŠ¨ã€‚
    å¦‚æœæ²¡æœ‰ï¼Œåˆ™è‡ªåŠ¨å¯åŠ¨å¹¶åœ¨ç¨‹åºé€€å‡ºæ—¶å…³é—­ã€‚
    """
    global _ollama_process, _ollama_started_by_app

    # 1. å°è¯•æ£€æŸ¥ Ollama æ˜¯å¦å·²è¿è¡Œ
    try:
        requests.get(f"{OLLAMA_URL}/api/tags", timeout=1)
        return  # å·²åœ¨è¿è¡Œ
    except:
        pass

    # 2. å¯åŠ¨ Ollama æœåŠ¡
    print("ğŸ”„ æ­£åœ¨å¯åŠ¨æœ¬åœ° Ollama æœåŠ¡å™¨...")
    _ollama_process = subprocess.Popen(
        ["ollama", "serve"],
        stdout=subprocess.DEVNULL,
        stderr=subprocess.STDOUT
    )
    _ollama_started_by_app = True

    # 3. ç­‰å¾… Ollama å°±ç»ª
    for _ in range(40):
        try:
            requests.get(f"{OLLAMA_URL}/api/tags", timeout=1)
            break
        except:
            time.sleep(0.25)
    else:
        raise RuntimeError("âŒ å¯åŠ¨ Ollama å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ˜¯å¦å·²å®‰è£…")

    # 4. è‡ªåŠ¨ pull æ¨¡å‹
    print(f"ğŸ“¥ æ­£åœ¨æ£€æŸ¥/æ‹‰å–æ¨¡å‹ {OLLAMA_MODEL}...")
    subprocess.run(["ollama", "pull", OLLAMA_MODEL], stdout=subprocess.DEVNULL)


# ---------------------------
# LLM è°ƒç”¨
# ---------------------------
def ask_llm(query: str, context: str) -> str:
    """å‘ LLM è¯¢é—®ç­”æ¡ˆï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ã€‚"""

    # ç¡®ä¿ Ollama å¯åŠ¨
    _start_ollama_if_needed()

    prompt = LLM_PROMPT.format(context=context, query=query)

    payload = {
        "model": OLLAMA_MODEL,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0,
            # ä½ è¦çš„å…¶ä»–å‚æ•°ä¹Ÿå¯ä»¥åŠ 
        }
    }

    try:
        resp = requests.post(f"{OLLAMA_URL}/api/generate", json=payload)
        resp.raise_for_status()
        data = resp.json()

        answer = data.get("response")
        if answer is None:
            return f"LLM è¿”å›å¼‚å¸¸ç»“æ„: {data}"
        return answer

    except Exception as e:
        return f"LLM è°ƒç”¨å¤±è´¥: {e}"
